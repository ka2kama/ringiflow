# ADR-020: サーキットブレーカーの状態管理方式

## ステータス

承認済み

## コンテキスト

### 背景

Auth Service 分離（[ADR-017](./017_AuthService分離の方針.md)）に伴い、BFF から Auth Service への通信にサーキットブレーカーを導入する。

サーキットブレーカーは依存先の障害を検知して通信を遮断するパターンだが、複数インスタンスで動作する場合に「状態をどこに持つか」という設計判断が必要になる。

### 問題

BFF は ECS Fargate で複数インスタンス（2-3 台）で動作する予定。各インスタンスが独立してサーキットブレーカーの状態を持つと：

```
Auth Service 障害発生
    ↓
BFF-1: 失敗 3 回 → Open
BFF-2: 失敗 1 回 → Closed（まだリクエストを通す）
BFF-3: 失敗 2 回 → Closed（まだリクエストを通す）
```

- インスタンスごとに挙動が異なる
- 障害検知が遅れる可能性（各インスタンスが個別に閾値に達するまで）
- ユーザー体験が不安定（リクエストごとに異なるインスタンスに振り分けられる）

## 検討した選択肢

### 選択肢 1: インスタンス単位で状態を持つ

各インスタンスがメモリ上で独立してサーキットブレーカーの状態を管理する。

```rust
// 各インスタンスが独立して持つ
struct CircuitBreaker {
    state: RwLock<CircuitState>,
    failure_count: AtomicU32,
    // ...
}
```

評価:
- 利点:
  - 実装がシンプル
  - 外部依存なし（Redis 不要）
  - レイテンシ増加なし
  - サーキットブレーカー自体が単一障害点にならない
- 欠点:
  - 障害検知が遅れる可能性
  - インスタンス間で状態が不整合

### 選択肢 2: Redis で状態を共有

Redis をバックエンドにして、全インスタンスで状態を共有する。

```rust
struct DistributedCircuitBreaker {
    redis: redis::Client,
    key: String,  // e.g., "circuit:auth-service"
}

impl DistributedCircuitBreaker {
    async fn record_failure(&self) -> Result<CircuitState> {
        // Redis でアトミックにインクリメント
        let count: u32 = redis::cmd("INCR")
            .arg(&format!("{}:failures", self.key))
            .query_async(&mut conn)
            .await?;
        // ...
    }
}
```

評価:
- 利点:
  - 全インスタンスで一貫した挙動
  - 障害検知が早い（失敗カウントが共有される）
- 欠点:
  - 毎回 Redis アクセスが発生（レイテンシ増加: 1-2ms）
  - Redis 自体の障害を考慮する必要がある
  - 実装が複雑化

### 選択肢 3: ハイブリッド（ローカル + 定期同期）

ローカルで状態を持ちつつ、定期的に Redis と同期する。

```rust
struct HybridCircuitBreaker {
    local_state: RwLock<CircuitState>,
    redis: redis::Client,
    sync_interval: Duration,  // e.g., 1秒
}
```

評価:
- 利点:
  - レイテンシ増加を最小限に抑えつつ、ある程度の一貫性を確保
- 欠点:
  - 実装が最も複雑
  - 同期のタイミングによっては不整合が発生
  - 本質的には選択肢 1 と 2 の中間で、どちらの問題も完全には解決しない

### 比較表

| 観点 | 選択肢 1: インスタンス単位 | 選択肢 2: Redis 共有 | 選択肢 3: ハイブリッド |
|------|--------------------------|---------------------|---------------------|
| 実装の複雑さ | **低** | 中 | 高 |
| レイテンシ影響 | **なし** | 1-2ms 増加 | 最小限 |
| 障害検知速度 | やや遅い | **早い** | 中間 |
| 外部依存 | **なし** | Redis | Redis |
| 単一障害点 | **なし** | Redis | Redis |

## 決定

**選択肢 1: インスタンス単位で状態を持つ** を採用する。

### 主な理由

1. **シンプルさの優先**
   - プロジェクト理念「必要十分な複雑さに留める」に合致
   - サーキットブレーカーのためだけに Redis 依存を追加する必要性が低い

2. **実用上の問題が小さい**
   - 3 インスタンスで閾値 3 回の場合、最悪でも 9 リクエストで全インスタンスが Open
   - Auth Service が本当に障害なら、数秒で全インスタンスが検知する
   - 「完璧な一貫性」より「実用的な遅延」を許容

3. **Redis 障害時のリスク回避**
   - Redis で状態を共有すると、Redis 障害時にサーキットブレーカー自体が動作しなくなる
   - 障害対策のための機構が新たな障害点を作るのは本末転倒

4. **業界標準**
   - Netflix Hystrix（サーキットブレーカーの先駆者）もデフォルトはインスタンス単位
   - 多くの実装がこの方式を採用している

### 将来の見直し条件

以下の状況になった場合、選択肢 2 または 3 への移行を検討する：

- BFF のインスタンス数が 10 台以上に増加
- Auth Service の障害頻度が高く、検知遅延が問題になる
- ユーザーから「ログインできたりできなかったりする」という報告が増加

## 帰結

### 肯定的な影響

- 実装がシンプルで保守しやすい
- サーキットブレーカー自体が障害点にならない
- レイテンシ増加なし

### 否定的な影響・トレードオフ

- 障害検知が最大で数秒遅れる可能性
- インスタンス間で一時的に挙動が異なる
- 監視・デバッグ時に各インスタンスの状態を個別に確認する必要がある

### 今後必要になる作業

1. サーキットブレーカーの実装（tower ミドルウェアまたは自前実装）
2. 各インスタンスの状態を確認できるヘルスチェックエンドポイントの追加
3. Open 状態への遷移をメトリクス化してダッシュボードに表示

### 関連ドキュメント

- [ナレッジベース: サーキットブレーカー](../06_ナレッジベース/architecture/サーキットブレーカー.md)
- [ADR-017: Auth Service 分離の方針](./017_AuthService分離の方針.md)
- [08_AuthService設計.md](../03_詳細設計書/08_AuthService設計.md)

---

## 変更履歴

| 日付 | 変更内容 |
|------|---------|
| 2026-01-22 | 初版作成 |
